{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Azure Machine Learning Pipelines\n\nThe goal of this quickstart is to build a pipeline that demonstrate the basic data science workflow of data preparation, model training, and predictions. Azure Machine Learning, allows you to define distinct steps and make it possible to rerun only the steps you need as you tweak and test your workflow.\n\nIn this quickstart, we will be using a subset of NYC Taxi & Limousine Commission - green taxi trip records available from [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/). The data is enriched with holiday and weather data. The goal is to train a regression model to predict taxi fares in New York City based on input features such as, number of passengers, trip distance, datetime, holiday information and weather information.\n\nThe machine learning pipeline in this quickstart is organized into three steps:\n\n- **Preprocess Training and Input Data:** We want to preprocess the data to better represent the datetime features, such as hours of the day, and day of the week to capture the cyclical nature of these features.\n\n- **Model Training:** We will use data transformations and the GradientBoostingRegressor algorithm from the scikit-learn library to train a regression model. The trained model will be saved for later making predictions.\n\n- **Model Inference:** In this scenario, we want to support **bulk predictions**. Each time an input file is uploaded to a data store, we want to initiate bulk model predictions on the input data. However, before we do model predictions, we will reuse the preprocess data step to process input data, and then make predictions on the processed data using the trained model.\n\nEach of these pipelines is going to have implicit data dependencies and the example will demonstrate how AML make it possible to reuse previously completed steps and run only the modified or new steps in your pipeline.\n\n**The pipelines will be run on the Azure Machine Learning compute.**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Azure Machine Learning and Pipeline SDK-specific Imports"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport azureml.core\nfrom azureml.core import Workspace, Experiment, Datastore\nfrom azureml.data.azure_storage_datastore import AzureBlobDatastore\nfrom azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nfrom azureml.widgets import RunDetails\n\n# Check core SDK version number\nprint(\"SDK version:\", azureml.core.VERSION)\n\nfrom azureml.data.data_reference import DataReference\nfrom azureml.pipeline.core import Pipeline, PipelineData\nfrom azureml.pipeline.steps import PythonScriptStep\nprint(\"Pipeline SDK-specific imports completed\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Setup\nTo begin, you will need to provide the following information about your Azure Subscription.\n\n**If you are using your own Azure subscription, please provide names for subscription_id, resource_group, workspace_name and workspace_region to use.** Note that the workspace needs to be of type [Machine Learning Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/setup-create-workspace).\n\n**If an environment is provided to you be sure to replace XXXXX in the values below with your unique identifier.**\n\nIn the following cell, be sure to set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments (*these values can be acquired from the Azure Portal*).\n\nTo get these values, do the following:\n1. Navigate to the Azure Portal and login with the credentials provided.\n2. From the left hand menu, under Favorites, select `Resource Groups`.\n3. In the list, select the resource group with the name similar to `XXXXX`.\n4. From the Overview tab, capture the desired values.\n\nExecute the following cell by selecting the `>|Run` button in the command bar above."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Provide the Subscription ID of your existing Azure subscription\nsubscription_id = \"\" # <- needs to be the subscription with the Quick-Starts resource group\n\n#Provide values for the existing Resource Group \nresource_group = \"Quick-Starts-XXXXX\" # <- replace XXXXX with your unique identifier\n\n#Provide the Workspace Name and Azure Region of the Azure Machine Learning Workspace\nworkspace_name = \"quick-starts-ws-XXXXX\" # <- replace XXXXX with your unique identifier\nworkspace_region = \"eastus\" # <- region of your Quick-Starts resource group\n\naml_compute_target = \"aml-compute\"\n\nexperiment_name = 'quick-starts-pipeline'\n\n# project folder for the script files\nproject_folder = 'aml-pipelines-scripts'\ndata_location = 'aml-pipelines-data'\ntest_data_location = 'aml-pipelines-test-data'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Download the training data and code for the pipeline steps\n\nOnce you download the files, please review the code for the three pipeline steps in the following files: `preprocess.py`, `train.py`, and `inference.py`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import urllib.request\nimport os\n\ndata_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n            'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n            'quickstarts/nyc-taxi-data/nyc-taxi-sample-data.csv')\n\npreprocess_script = ('https://quickstartsws9073123377.blob.core.windows.net/'\n                     'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n                     'quickstarts/pipeline-scripts/preprocess.py')\n                     \ntrain_script = ('https://quickstartsws9073123377.blob.core.windows.net/'\n                'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n                'quickstarts/pipeline-scripts/train.py')\n                \ninference_script = ('https://quickstartsws9073123377.blob.core.windows.net/'\n                    'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n                    'quickstarts/pipeline-scripts/inference.py')\n\n\n# Download the raw training data to your local disk\nos.makedirs(data_location, exist_ok=True)\nurllib.request.urlretrieve(data_url, os.path.join(data_location, 'nyc-taxi-sample-data.csv'))\n\n# Download the script files to your local disk\nos.makedirs(project_folder, exist_ok=True)\nurllib.request.urlretrieve(preprocess_script, os.path.join(project_folder, 'preprocess.py'))\nurllib.request.urlretrieve(train_script, os.path.join(project_folder, 'train.py'))\nurllib.request.urlretrieve(inference_script, os.path.join(project_folder, 'inference.py'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create and connect to an Azure Machine Learning Workspace\n\nRun the following cell to create a new Azure Machine Learning **Workspace** and save the configuration to disk (next to the Jupyter notebook). \n\n**Important Note**: You will be prompted to login in the text that is output below the cell. Be sure to navigate to the URL displayed and enter the code that is provided. Once you have entered the code, return to this notebook and wait for the output to read `Workspace configuration succeeded`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# By using the exist_ok param, if the worskpace already exists you get a reference to the existing workspace\n# allowing you to re-run this cell multiple times as desired (which is fairly common in notebooks).\nws = Workspace.create(\n    name = workspace_name,\n    subscription_id = subscription_id,\n    resource_group = resource_group, \n    location = workspace_region,\n    exist_ok = True)\n\nws.write_config()\nprint('Workspace configuration succeeded')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create AML Compute Cluster\n\nAzure Machine Learning Compute is a service for provisioning and managing clusters of Azure virtual machines for running machine learning workloads. Let's create a new Aml Compute in the current workspace, if it doesn't already exist. We will run all our pipelines on this compute target."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute_target import ComputeTargetException\n\ntry:\n    aml_compute = AmlCompute(ws, aml_compute_target)\n    print(\"found existing compute target.\")\nexcept ComputeTargetException:\n    print(\"creating new compute target\")\n    \n    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n                                                                min_nodes = 1, \n                                                                max_nodes = 1)    \n    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n    \nprint(\"Aml Compute attached\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create the Run Configuration"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.runconfig import DEFAULT_CPU_IMAGE\n\n# Create a new runconfig object\nrun_amlcompute = RunConfiguration()\n\n# Use the cpu_cluster you created above. \nrun_amlcompute.target = aml_compute_target\n\n# Enable Docker\nrun_amlcompute.environment.docker.enabled = True\n\n# Set Docker base image to the default CPU-based image\nrun_amlcompute.environment.docker.base_image = DEFAULT_CPU_IMAGE\n\n# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\nrun_amlcompute.environment.python.user_managed_dependencies = False\n\n# Auto-prepare the Docker image when used for execution (if it is not already prepared)\nrun_amlcompute.auto_prepare_environment = True\n\n# Specify CondaDependencies obj, add necessary packages\nrun_amlcompute.environment.python.conda_dependencies = CondaDependencies.create(pip_packages=[\n    'numpy',\n    'pandas',\n    'scikit-learn',\n    'sklearn_pandas'\n])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### The Process Training Data Pipeline Step"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The process training data step in the pipeline takes raw training data as input. This data can be a data source that lives in one of the accessible data locations, or intermediate data produced by a previous step in the pipeline. In this example we will upload the raw training data in the workspace's default blob store. Run the following two cells at the end of which we will create a **DataReference** object that points to the raw training data *csv* file stored in the default blob store."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Default datastore (Azure file storage)\ndef_file_store = ws.get_default_datastore() \nprint(\"Default datastore's name: {}\".format(def_file_store.name))\ndef_blob_store = Datastore(ws, \"workspaceblobstore\")\nprint(\"Blobstore's name: {}\".format(def_blob_store.name))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Upload the raw training data to the blob storage\ndef_blob_store.upload(src_dir=data_location, \n                      target_path='nyc-taxi-raw-features', \n                      overwrite=True, \n                      show_progress=True)\n\nraw_train_data = DataReference(datastore=def_blob_store, \n                                      data_reference_name=\"nyc_taxi_raw_features\", \n                                      path_on_datastore=\"nyc-taxi-raw-features/nyc-taxi-sample-data.csv\")\nprint(\"DataReference object created\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create the Process Training Data Pipeline Step"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The intermediate data (or output of a Step) is represented by PipelineData object. PipelineData can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps.\n\nThe process training data pipeline step takes the raw_train_data DataReference object as input, and it will output an intermediate PipelineData object that holds the processed training data with the new engineered features for datetime components: hour of the day, and day of the week.\n\nReview and run the cell below to construct the PipelineData objects and the PythonScriptStep pipeline step:\n\n*Open preprocess.py in the local machine and examine the arguments, inputs, and outputs for the script. Note that there is an argument called process_mode to distinguish between processing training data vs test data. Reviewing the Python script file will give you a good sense of why the script argument names used below are important.*"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "processed_train_data = PipelineData('processed_train_data', datastore=def_blob_store)\nprint(\"PipelineData object created\")\n\nprocessTrainDataStep = PythonScriptStep(\n    name=\"process_train_data\",\n    script_name=\"preprocess.py\", \n    arguments=[\"--process_mode\", 'train',\n               \"--input\", raw_train_data,\n               \"--output\", processed_train_data],\n    inputs=[raw_train_data],\n    outputs=[processed_train_data],\n    compute_target=aml_compute,\n    runconfig=run_amlcompute,\n    source_directory=project_folder\n)\nprint(\"preprocessStep created\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create the Train Pipeline Step"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The train pipeline step takes the *processed_train_data* created in the above step as input and generates another PipelineData object to save the *trained_model* as its output. This is an example of how machine learning pipelines can have many steps and these steps could use or reuse datasources and intermediate data.\n\n*Open train.py in the local machine and examine the arguments, inputs, and outputs for the script.*"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Challenge Task\n\nIn the cell below create the train pipeline with the following parameters:\n\n1. name: train\n2. script_name: train.py\n3. arguments: \"--input\", processed_train_data, \"--output\", trained_model\n4. inputs\n5. outputs\n6. compute_target\n7. runconfig\n8. source_directory: project_folder\n\nWhere it takes the PipelineData: `processed_train_data` as inputs and PipelineData: `trained_model` as outputs\n\n*A complete solution can be found in the accompanying notebook: `solution-pipelines-AML.ipynb`*"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "trained_model = PipelineData('trained_model', datastore=def_blob_store)\nprint(\"PipelineData object created\")\n\n# Insert your code here...\ntrainStep = PythonScriptStep(\n)\n\nprint(\"trainStep created\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create and Validate the Pipeline\n\nNote that the *trainStep* has implicit data dependency with the *processTrainDataStep* and thus you only include the *trainStep* in your Pipeline object. You will observe that when you run the pipeline that it will first run the **processTrainDataStep** followed by the **trainStep**."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pipeline = Pipeline(workspace=ws, steps=[trainStep])\nprint (\"Pipeline is built\")\n\npipeline.validate()\nprint(\"Simple validation complete\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Submit the Pipeline\n\nAt this point you can run the pipeline and examine the output it produced."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pipeline_run = Experiment(ws, experiment_name).submit(pipeline)\nprint(\"Pipeline is submitted for execution\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Monitor the Run Details\n\nObserve the order in which the pipeline steps are executed: **processTrainDataStep** followed by the **trainStep**\n\nWait till both steps finish running. The cell below should periodically auto-refresh and you can also rerun the cell to force a refresh."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "RunDetails(pipeline_run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Bulk Predictions"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create the Raw Test Data DataReference Object\n\nCreate the **DataReference** object where the raw bulk test or input data file will be uploaded."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Upload dummy raw test data to the blob storage\nos.makedirs(test_data_location, exist_ok=True)\npd.DataFrame([[0]], columns = ['col1']).to_csv(os.path.join(test_data_location, 'raw-test-data.csv'), header=True, index=False)\n\ndef_blob_store.upload(src_dir=test_data_location, \n                      target_path='bulk-test-data', \n                      overwrite=True, \n                      show_progress=True)\n\n# Create a DataReference object referencing the 'raw-test-data.csv' file\nraw_bulk_test_data = DataReference(datastore=def_blob_store, \n                                      data_reference_name=\"raw_bulk_test_data\", \n                                      path_on_datastore=\"bulk-test-data/raw-test-data.csv\")\nprint(\"DataReference object created\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create the Process Test Data Pipeline Step"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Similar to the process train data pipeline step, we create a new step for processing the test data. Note that it is the same script file *preprocess.py* that is used to process both the train and test data. Thus, you can centralize all your logic for preprocessing data for both train and test. The key differences here is that the process_mode argument is set to *inference*. \n\n*Open preprocess.py in the local machine and examine the arguments, inputs, and outputs for the script. Note that there is an argument called process_mode to distinguish between processing training data vs test data. Reviewing the Python script file will give you a good sense of why the script argument names used below are important.*"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "processed_test_data = PipelineData('processed_test_data', datastore=def_blob_store)\nprint(\"PipelineData object created\")\n\nprocessTestDataStep = PythonScriptStep(\n    name=\"process_test_data\",\n    script_name=\"preprocess.py\", \n    arguments=[\"--process_mode\", 'inference',\n               \"--input\", raw_bulk_test_data,\n               \"--output\", processed_test_data],\n    inputs=[raw_bulk_test_data],\n    outputs=[processed_test_data],\n    allow_reuse = False,\n    compute_target=aml_compute,\n    runconfig=run_amlcompute,\n    source_directory=project_folder\n)\nprint(\"preprocessStep created\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create the Inference Pipeline Step to Make Predictions"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The inference pipeline step takes the *processed_test_data* created in the above step and the *trained_model* created in the train step as two inputs and generates *inference_output* as its output. This is yet another example of how machine learning pipelines can have many steps and these steps could use or reuse datasources and intermediate data.\n\n*Open inference.py in the local machine and examine the arguments, inputs, and outputs for the script.*"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "inference_output = PipelineData('inference_output', datastore=def_blob_store)\nprint(\"PipelineData object created\")\n\ninferenceStep = PythonScriptStep(\n    name=\"inference\",\n    script_name=\"inference.py\", \n    arguments=[\"--input\", processed_test_data,\n               \"--model\", trained_model,\n               \"--output\", inference_output],\n    inputs=[processed_test_data, trained_model],\n    outputs=[inference_output],\n    compute_target=aml_compute,\n    runconfig=run_amlcompute,\n    source_directory=project_folder\n)\nprint(\"inferenceStep created\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create and Validate the Pipeline\n\nNote that the *inferenceStep* has implicit data dependency with **ALL** of the previous pipeline steps."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "inference_pipeline = Pipeline(workspace=ws, steps=[inferenceStep])\nprint (\"Inference Pipeline is built\")\n\ninference_pipeline.validate()\nprint(\"Simple validation complete\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Publish the Inference Pipeline\n\nNote that we are not submitting the pipeline to run, instead we are publishing the pipeline. When you publish a pipeline, it can be submitted to run without the Python code which constructed the Pipeline."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pipeline_name = 'Inference Pipeline'\npublished_pipeline = inference_pipeline.publish(name = pipeline_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Schedule the Inference Pipeline\n\nWe want to run the Inference Pipeline when a new test or input data is uploaded at the location referenced by the `raw_bulk_test_data` DataReference object. The next cell creates a Schedule to monitor the datastore for changes, and is responsible for running the `Inference Pipeline` when it detects a new file being uploaded.\n\n**Please disable the Schedule after completing the quickstart. To disable the Schedule, run the `Cleanup Resources` section at the end of the notebook.**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.pipeline.core.schedule import Schedule\n\nschedule = Schedule.create(workspace=ws, name=pipeline_name + \"_sch\",\n                           pipeline_id=published_pipeline.id, \n                           experiment_name=experiment_name,\n                           datastore=def_blob_store,\n                           wait_for_provisioning=True,\n                           description=\"Datastore scheduler for Pipeline: \" + pipeline_name,\n                           path_on_datastore='bulk-test-data',\n                           polling_interval=1 # in minutes\n                           )\n\nprint(\"Created schedule with id: {}\".format(schedule.id))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Check the status of the Schedule and confirm it's Active\nprint('Schedule status: ', schedule.status)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Test the Inference Pipeline\n\nCreate some test data to make bulk predictions. Upload the test data to the `bulk-test-data` blob store. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Download the raw training data to your local disk\ncolumns = ['vendorID', 'passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', 'day_of_month', \n           'month_num', 'normalizeHolidayName', 'isPaidTimeOff', 'snowDepth', 'precipTime', \n           'precipDepth', 'temperature']\n\ndata = [[1, 4, 10, 15, 4, 5, 7, 'None', False, 0, 0.0, 0.0, 80], \n        [1, 1, 5, 6, 0, 20, 1, 'Martin Luther King, Jr. Day', True, 0, 2.0, 3.0, 35]]\n\ndata_df = pd.DataFrame(data, columns = columns)\n\nos.makedirs(test_data_location, exist_ok=True)\ndata_df.to_csv(os.path.join(test_data_location, 'raw-test-data.csv'), header=True, index=False)\n\nfrom datetime import datetime\ndata_upload_time = datetime.utcnow()\nprint('Data upload time in UTC: ', data_upload_time)\n\n# Upload the raw test data to the blob storage\ndef_blob_store.upload(src_dir=test_data_location, \n                      target_path='bulk-test-data', \n                      overwrite=True, \n                      show_progress=True)\n\n# Wait for 65 seconds...\nimport time\nprint('Please wait...')\ntime.sleep(65)\nprint('Done!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Wait for Schedule to Trigger\n\nSchedule's polling interval is 1 minute. You can also log into Azure Portal and navigate to your `resource group -> workspace -> experiment` to see if the `Inference Pipeline` has started executing.\n\n**If the inference_pipeline_run object in the below cell is None, it means that the Schedule has not triggered yet!**\n\n**If the Schedule does not trigger in 2 minutes, try rerunning the data upload cell again!**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Confirm that the inference_pipeline_run object is NOT None\ninference_pipeline_run = schedule.get_last_pipeline_run()\nprint(inference_pipeline_run)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*If upload the test data file more than once, confirm that we have the latest pipeline run object. We will compare the pipeline start time with the time you uploaded the test data file.*"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# confirm the start time\nimport dateutil.parser\n\nif inference_pipeline_run.get_details()['status'] != 'NotStarted':\n    pipeline_starttime = dateutil.parser.parse(inference_pipeline_run.get_details()['startTimeUtc'], ignoretz=True)\nelse:\n    pipeline_starttime = datetime.utcnow()\n\nif(pipeline_starttime > data_upload_time):\n    print('We have the correct inference pipeline run! Proceed to next cell.')\nelse:\n    print('Rerun the above cell to get the latest inference pipeline run!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Monitor the Run Details\n\nObserve the order in which the pipeline steps are executed based on their implicit data dependencies.\n\nWait till all steps finish running. The cell below should periodically auto-refresh and you can also rerun the cell to force a refresh.\n\n**This example demonstrates how AML make it possible to reuse previously completed steps and run only the modified or new steps in your pipeline.**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "RunDetails(inference_pipeline_run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Download and Observe the Predictions"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# access the inference_output\ndata = inference_pipeline_run.find_step_run('inference')[0].get_output_data('inference_output')\n# download the predictions to local path\ndata.download('.', show_progress=True)\n# print the predictions\npredictions = np.loadtxt(os.path.join('./', data.path_on_datastore, 'results.txt'))\nprint(predictions)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Cleanup Resources\n\nIf you are done experimenting with this quickstart, run the following two cell to clean up resources."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "schedule.disable()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "aml_compute.delete()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}